{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**21-03-24 ML_DL 10_DNN iris(다중분류) (c)cherryuki (ji)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. DNN iris (다중분류)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns #아이리스 데이터\n",
    "import pandas as pd #원핫인코딩\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split #훈련셋과 테스트셋 분리\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((105, 4), (105, 3), (45, 4), (45, 3))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1. 데이터 셋\n",
    "iris = sns.load_dataset('iris')\n",
    "#display(iris.head())\n",
    "iris_X = iris.iloc[:, :-1].to_numpy()\n",
    "iris_Y = iris.iloc[:, -1]\n",
    "#iris_Y 원핫 인코딩(문자이므로 pd.get_dummies()이용) \n",
    "##utils.to_categorical()은 숫자만 가능\n",
    "iris_Y = pd.get_dummies(iris_Y).to_numpy()\n",
    "# 1 0 0 -> setosa\n",
    "# 0 1 0 -> versicolor\n",
    "# 0 0 1 -> virginica\n",
    "#훈련 데이터와 시험 데이터 분리\n",
    "train_X,test_X, train_Y, test_Y = train_test_split(iris_X, iris_Y,\n",
    "                                                  test_size=0.3, random_state=1)\n",
    "train_X.shape, train_Y.shape, test_X.shape, test_Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 1.2092 - accuracy: 0.2600\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.04762, saving model to ./model\\iris-001-val0.0476.h5\n",
      "2/2 [==============================] - 0s 123ms/step - loss: 1.1985 - accuracy: 0.2619 - val_loss: 1.0748 - val_accuracy: 0.0476\n",
      "Epoch 2/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 1.0965 - accuracy: 0.1200\n",
      "Epoch 00002: val_accuracy improved from 0.04762 to 0.28571, saving model to ./model\\iris-002-val0.2857.h5\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 1.0994 - accuracy: 0.2262 - val_loss: 1.0858 - val_accuracy: 0.2857\n",
      "Epoch 3/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 1.0333 - accuracy: 0.4200\n",
      "Epoch 00003: val_accuracy did not improve from 0.28571\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 1.0494 - accuracy: 0.3690 - val_loss: 1.1138 - val_accuracy: 0.2857\n",
      "Epoch 4/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 1.0153 - accuracy: 0.3600\n",
      "Epoch 00004: val_accuracy did not improve from 0.28571\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 1.0088 - accuracy: 0.3690 - val_loss: 1.1308 - val_accuracy: 0.2857\n",
      "Epoch 5/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.9862 - accuracy: 0.3400\n",
      "Epoch 00005: val_accuracy improved from 0.28571 to 0.42857, saving model to ./model\\iris-005-val0.4286.h5\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.9832 - accuracy: 0.3690 - val_loss: 1.1214 - val_accuracy: 0.4286\n",
      "Epoch 6/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.9510 - accuracy: 0.5600\n",
      "Epoch 00006: val_accuracy improved from 0.42857 to 0.52381, saving model to ./model\\iris-006-val0.5238.h5\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.9541 - accuracy: 0.6310 - val_loss: 1.0887 - val_accuracy: 0.5238\n",
      "Epoch 7/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.8772 - accuracy: 0.8000\n",
      "Epoch 00007: val_accuracy did not improve from 0.52381\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.9248 - accuracy: 0.7381 - val_loss: 1.0471 - val_accuracy: 0.5238\n",
      "Epoch 8/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.8775 - accuracy: 0.7800\n",
      "Epoch 00008: val_accuracy did not improve from 0.52381\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.8943 - accuracy: 0.7381 - val_loss: 0.9983 - val_accuracy: 0.5238\n",
      "Epoch 9/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.8637 - accuracy: 0.7600\n",
      "Epoch 00009: val_accuracy did not improve from 0.52381\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.8657 - accuracy: 0.7381 - val_loss: 0.9509 - val_accuracy: 0.5238\n",
      "Epoch 10/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.8297 - accuracy: 0.7600\n",
      "Epoch 00010: val_accuracy did not improve from 0.52381\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.8363 - accuracy: 0.7381 - val_loss: 0.9121 - val_accuracy: 0.5238\n",
      "Epoch 11/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.8261 - accuracy: 0.7000\n",
      "Epoch 00011: val_accuracy did not improve from 0.52381\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.8087 - accuracy: 0.7381 - val_loss: 0.8809 - val_accuracy: 0.5238\n",
      "Epoch 12/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.7555 - accuracy: 0.8000\n",
      "Epoch 00012: val_accuracy did not improve from 0.52381\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.7801 - accuracy: 0.7381 - val_loss: 0.8612 - val_accuracy: 0.5238\n",
      "Epoch 13/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.7725 - accuracy: 0.7200\n",
      "Epoch 00013: val_accuracy did not improve from 0.52381\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.7479 - accuracy: 0.7381 - val_loss: 0.8435 - val_accuracy: 0.5238\n",
      "Epoch 14/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.7157 - accuracy: 0.7600\n",
      "Epoch 00014: val_accuracy did not improve from 0.52381\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.7189 - accuracy: 0.7381 - val_loss: 0.8331 - val_accuracy: 0.5238\n",
      "Epoch 15/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.7144 - accuracy: 0.7000\n",
      "Epoch 00015: val_accuracy did not improve from 0.52381\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.6885 - accuracy: 0.7381 - val_loss: 0.8180 - val_accuracy: 0.5238\n",
      "Epoch 16/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.6423 - accuracy: 0.7800\n",
      "Epoch 00016: val_accuracy did not improve from 0.52381\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.6628 - accuracy: 0.7381 - val_loss: 0.8045 - val_accuracy: 0.5238\n",
      "Epoch 17/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.6108 - accuracy: 0.7800\n",
      "Epoch 00017: val_accuracy did not improve from 0.52381\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.6361 - accuracy: 0.7381 - val_loss: 0.7789 - val_accuracy: 0.5238\n",
      "Epoch 18/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5748 - accuracy: 0.8000\n",
      "Epoch 00018: val_accuracy did not improve from 0.52381\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.6092 - accuracy: 0.7381 - val_loss: 0.7484 - val_accuracy: 0.5238\n",
      "Epoch 19/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.4883 - accuracy: 0.8800\n",
      "Epoch 00019: val_accuracy did not improve from 0.52381\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.5830 - accuracy: 0.7381 - val_loss: 0.7175 - val_accuracy: 0.5238\n",
      "Epoch 20/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5527 - accuracy: 0.7600\n",
      "Epoch 00020: val_accuracy improved from 0.52381 to 0.61905, saving model to ./model\\iris-020-val0.6190.h5\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.5583 - accuracy: 0.7381 - val_loss: 0.6826 - val_accuracy: 0.6190\n",
      "Epoch 21/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5301 - accuracy: 0.7800\n",
      "Epoch 00021: val_accuracy improved from 0.61905 to 0.71429, saving model to ./model\\iris-021-val0.7143.h5\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.5371 - accuracy: 0.7857 - val_loss: 0.6561 - val_accuracy: 0.7143\n",
      "Epoch 22/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5573 - accuracy: 0.7400\n",
      "Epoch 00022: val_accuracy improved from 0.71429 to 0.76190, saving model to ./model\\iris-022-val0.7619.h5\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.5184 - accuracy: 0.8214 - val_loss: 0.6338 - val_accuracy: 0.7619\n",
      "Epoch 23/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.4948 - accuracy: 0.8800\n",
      "Epoch 00023: val_accuracy did not improve from 0.76190\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.4958 - accuracy: 0.8690 - val_loss: 0.6235 - val_accuracy: 0.7143\n",
      "Epoch 24/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.4582 - accuracy: 0.8800\n",
      "Epoch 00024: val_accuracy did not improve from 0.76190\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.4747 - accuracy: 0.8333 - val_loss: 0.6186 - val_accuracy: 0.6667\n",
      "Epoch 25/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5065 - accuracy: 0.7000\n",
      "Epoch 00025: val_accuracy did not improve from 0.76190\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.4559 - accuracy: 0.7857 - val_loss: 0.6082 - val_accuracy: 0.6667\n",
      "Epoch 26/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.3896 - accuracy: 0.8400\n",
      "Epoch 00026: val_accuracy did not improve from 0.76190\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.4419 - accuracy: 0.7619 - val_loss: 0.6015 - val_accuracy: 0.6190\n",
      "Epoch 27/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.4210 - accuracy: 0.7800\n",
      "Epoch 00027: val_accuracy did not improve from 0.76190\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.4243 - accuracy: 0.7619 - val_loss: 0.5773 - val_accuracy: 0.7143\n",
      "Epoch 28/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.4506 - accuracy: 0.7800\n",
      "Epoch 00028: val_accuracy improved from 0.76190 to 0.80952, saving model to ./model\\iris-028-val0.8095.h5\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.4108 - accuracy: 0.8571 - val_loss: 0.5450 - val_accuracy: 0.8095\n",
      "Epoch 29/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/2 [==============>...............] - ETA: 0s - loss: 0.3680 - accuracy: 0.9200\n",
      "Epoch 00029: val_accuracy improved from 0.80952 to 0.85714, saving model to ./model\\iris-029-val0.8571.h5\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.3931 - accuracy: 0.8929 - val_loss: 0.5230 - val_accuracy: 0.8571\n",
      "Epoch 30/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.4130 - accuracy: 0.8600\n",
      "Epoch 00030: val_accuracy did not improve from 0.85714\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.3817 - accuracy: 0.9048 - val_loss: 0.5028 - val_accuracy: 0.8571\n",
      "Epoch 31/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.3784 - accuracy: 0.9200\n",
      "Epoch 00031: val_accuracy did not improve from 0.85714\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.3673 - accuracy: 0.9286 - val_loss: 0.4925 - val_accuracy: 0.8571\n",
      "Epoch 32/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.3413 - accuracy: 0.9400\n",
      "Epoch 00032: val_accuracy did not improve from 0.85714\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.3574 - accuracy: 0.9286 - val_loss: 0.4857 - val_accuracy: 0.8571\n",
      "Epoch 33/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.3379 - accuracy: 0.9200\n",
      "Epoch 00033: val_accuracy did not improve from 0.85714\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.3437 - accuracy: 0.9286 - val_loss: 0.4688 - val_accuracy: 0.8571\n",
      "Epoch 34/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.3331 - accuracy: 0.9400\n",
      "Epoch 00034: val_accuracy did not improve from 0.85714\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.3326 - accuracy: 0.9405 - val_loss: 0.4516 - val_accuracy: 0.8571\n",
      "Epoch 35/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.3425 - accuracy: 0.9400\n",
      "Epoch 00035: val_accuracy did not improve from 0.85714\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.3227 - accuracy: 0.9524 - val_loss: 0.4345 - val_accuracy: 0.8571\n",
      "Epoch 36/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.2896 - accuracy: 0.9800\n",
      "Epoch 00036: val_accuracy did not improve from 0.85714\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.3125 - accuracy: 0.9524 - val_loss: 0.4230 - val_accuracy: 0.8571\n",
      "Epoch 37/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.2948 - accuracy: 0.9800\n",
      "Epoch 00037: val_accuracy did not improve from 0.85714\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.3033 - accuracy: 0.9524 - val_loss: 0.4131 - val_accuracy: 0.8571\n",
      "Epoch 38/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.2965 - accuracy: 0.9400\n",
      "Epoch 00038: val_accuracy did not improve from 0.85714\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.2933 - accuracy: 0.9524 - val_loss: 0.4012 - val_accuracy: 0.8571\n",
      "Epoch 39/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.2521 - accuracy: 1.0000\n",
      "Epoch 00039: val_accuracy improved from 0.85714 to 0.90476, saving model to ./model\\iris-039-val0.9048.h5\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.2847 - accuracy: 0.9524 - val_loss: 0.3901 - val_accuracy: 0.9048\n",
      "Epoch 40/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.3138 - accuracy: 0.9400\n",
      "Epoch 00040: val_accuracy did not improve from 0.90476\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.2776 - accuracy: 0.9643 - val_loss: 0.3769 - val_accuracy: 0.9048\n",
      "Epoch 41/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.2632 - accuracy: 0.9800\n",
      "Epoch 00041: val_accuracy did not improve from 0.90476\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.2673 - accuracy: 0.9762 - val_loss: 0.3733 - val_accuracy: 0.9048\n",
      "Epoch 42/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.2469 - accuracy: 0.9800\n",
      "Epoch 00042: val_accuracy did not improve from 0.90476\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.2611 - accuracy: 0.9643 - val_loss: 0.3698 - val_accuracy: 0.8571\n",
      "Epoch 43/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.2538 - accuracy: 0.9800\n",
      "Epoch 00043: val_accuracy did not improve from 0.90476\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2517 - accuracy: 0.9643 - val_loss: 0.3542 - val_accuracy: 0.9048\n",
      "Epoch 44/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.2452 - accuracy: 0.9600\n",
      "Epoch 00044: val_accuracy did not improve from 0.90476\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.2447 - accuracy: 0.9762 - val_loss: 0.3375 - val_accuracy: 0.9048\n",
      "Epoch 45/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.2340 - accuracy: 0.9600\n",
      "Epoch 00045: val_accuracy did not improve from 0.90476\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2367 - accuracy: 0.9762 - val_loss: 0.3290 - val_accuracy: 0.9048\n",
      "Epoch 46/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.2571 - accuracy: 0.9800\n",
      "Epoch 00046: val_accuracy did not improve from 0.90476\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2296 - accuracy: 0.9762 - val_loss: 0.3231 - val_accuracy: 0.9048\n",
      "Epoch 47/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.2041 - accuracy: 0.9800\n",
      "Epoch 00047: val_accuracy did not improve from 0.90476\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.2216 - accuracy: 0.9762 - val_loss: 0.3159 - val_accuracy: 0.9048\n",
      "Epoch 48/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.2184 - accuracy: 0.9800\n",
      "Epoch 00048: val_accuracy did not improve from 0.90476\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.2111 - accuracy: 0.9762 - val_loss: 0.2963 - val_accuracy: 0.9048\n",
      "Epoch 49/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.2142 - accuracy: 0.9800\n",
      "Epoch 00049: val_accuracy did not improve from 0.90476\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.2030 - accuracy: 0.9762 - val_loss: 0.2865 - val_accuracy: 0.9048\n",
      "Epoch 50/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1589 - accuracy: 1.0000\n",
      "Epoch 00050: val_accuracy did not improve from 0.90476\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1971 - accuracy: 0.9762 - val_loss: 0.2852 - val_accuracy: 0.9048\n",
      "Epoch 51/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.2059 - accuracy: 0.9600\n",
      "Epoch 00051: val_accuracy did not improve from 0.90476\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1905 - accuracy: 0.9762 - val_loss: 0.2800 - val_accuracy: 0.9048\n",
      "Epoch 52/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.2041 - accuracy: 0.9600\n",
      "Epoch 00052: val_accuracy did not improve from 0.90476\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1844 - accuracy: 0.9762 - val_loss: 0.2779 - val_accuracy: 0.9048\n",
      "Epoch 53/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1914 - accuracy: 0.9800\n",
      "Epoch 00053: val_accuracy did not improve from 0.90476\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.1798 - accuracy: 0.9762 - val_loss: 0.2713 - val_accuracy: 0.9048\n",
      "Epoch 54/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1714 - accuracy: 0.9800\n",
      "Epoch 00054: val_accuracy did not improve from 0.90476\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.1741 - accuracy: 0.9762 - val_loss: 0.2650 - val_accuracy: 0.9048\n",
      "Epoch 55/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1709 - accuracy: 0.9800\n",
      "Epoch 00055: val_accuracy did not improve from 0.90476\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.1685 - accuracy: 0.9762 - val_loss: 0.2566 - val_accuracy: 0.9048\n",
      "Epoch 56/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1281 - accuracy: 0.9800\n",
      "Epoch 00056: val_accuracy did not improve from 0.90476\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.1637 - accuracy: 0.9762 - val_loss: 0.2466 - val_accuracy: 0.9048\n",
      "Epoch 57/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1798 - accuracy: 0.9600\n",
      "Epoch 00057: val_accuracy did not improve from 0.90476\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.1586 - accuracy: 0.9762 - val_loss: 0.2402 - val_accuracy: 0.9048\n",
      "Epoch 58/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1683 - accuracy: 0.9800\n",
      "Epoch 00058: val_accuracy did not improve from 0.90476\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.1545 - accuracy: 0.9762 - val_loss: 0.2386 - val_accuracy: 0.9048\n",
      "Epoch 59/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1353 - accuracy: 0.9800\n",
      "Epoch 00059: val_accuracy did not improve from 0.90476\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.1499 - accuracy: 0.9762 - val_loss: 0.2319 - val_accuracy: 0.9048\n",
      "Epoch 60/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1358 - accuracy: 0.9800\n",
      "Epoch 00060: val_accuracy did not improve from 0.90476\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.1472 - accuracy: 0.9762 - val_loss: 0.2243 - val_accuracy: 0.9048\n",
      "Epoch 61/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1425 - accuracy: 0.9800\n",
      "Epoch 00061: val_accuracy did not improve from 0.90476\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1429 - accuracy: 0.9881 - val_loss: 0.2133 - val_accuracy: 0.9048\n",
      "Epoch 62/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1329 - accuracy: 0.9800\n",
      "Epoch 00062: val_accuracy did not improve from 0.90476\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1395 - accuracy: 0.9881 - val_loss: 0.2134 - val_accuracy: 0.9048\n",
      "Epoch 63/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1240 - accuracy: 0.9800\n",
      "Epoch 00063: val_accuracy did not improve from 0.90476\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1348 - accuracy: 0.9762 - val_loss: 0.2199 - val_accuracy: 0.9048\n",
      "Epoch 64/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1480 - accuracy: 0.9600\n",
      "Epoch 00064: val_accuracy did not improve from 0.90476\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.1320 - accuracy: 0.9762 - val_loss: 0.2213 - val_accuracy: 0.9048\n",
      "Epoch 65/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1300 - accuracy: 0.9800\n",
      "Epoch 00065: val_accuracy did not improve from 0.90476\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.1294 - accuracy: 0.9762 - val_loss: 0.2105 - val_accuracy: 0.9048\n",
      "Epoch 66/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1092 - accuracy: 0.9800\n",
      "Epoch 00066: val_accuracy did not improve from 0.90476\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.1259 - accuracy: 0.9762 - val_loss: 0.2023 - val_accuracy: 0.9048\n",
      "Epoch 67/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1500 - accuracy: 0.9800\n",
      "Epoch 00067: val_accuracy did not improve from 0.90476\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.1226 - accuracy: 0.9881 - val_loss: 0.1997 - val_accuracy: 0.9048\n",
      "Epoch 68/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0714 - accuracy: 1.0000\n",
      "Epoch 00068: val_accuracy did not improve from 0.90476\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1220 - accuracy: 0.9762 - val_loss: 0.1998 - val_accuracy: 0.9048\n",
      "Epoch 69/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0942 - accuracy: 0.9800\n",
      "Epoch 00069: val_accuracy did not improve from 0.90476\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1174 - accuracy: 0.9762 - val_loss: 0.1919 - val_accuracy: 0.9048\n",
      "Epoch 70/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1137 - accuracy: 0.9800\n",
      "Epoch 00070: val_accuracy did not improve from 0.90476\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1154 - accuracy: 0.9881 - val_loss: 0.1875 - val_accuracy: 0.9048\n",
      "Epoch 71/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0960 - accuracy: 1.0000\n",
      "Epoch 00071: val_accuracy did not improve from 0.90476\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.1141 - accuracy: 0.9881 - val_loss: 0.1897 - val_accuracy: 0.9048\n",
      "Epoch 72/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1297 - accuracy: 0.9800\n",
      "Epoch 00072: val_accuracy did not improve from 0.90476\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.1115 - accuracy: 0.9881 - val_loss: 0.1886 - val_accuracy: 0.9048\n",
      "Epoch 73/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1255 - accuracy: 0.9600\n",
      "Epoch 00073: val_accuracy did not improve from 0.90476\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1095 - accuracy: 0.9762 - val_loss: 0.1915 - val_accuracy: 0.9048\n",
      "Epoch 74/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1000 - accuracy: 0.9800\n",
      "Epoch 00074: val_accuracy did not improve from 0.90476\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.1075 - accuracy: 0.9762 - val_loss: 0.1969 - val_accuracy: 0.9048\n",
      "Epoch 75/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0911 - accuracy: 0.9800\n",
      "Epoch 00075: val_accuracy did not improve from 0.90476\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1079 - accuracy: 0.9762 - val_loss: 0.1908 - val_accuracy: 0.9048\n",
      "Epoch 76/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0599 - accuracy: 1.0000\n",
      "Epoch 00076: val_accuracy did not improve from 0.90476\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.1032 - accuracy: 0.9762 - val_loss: 0.1757 - val_accuracy: 0.9048\n",
      "Epoch 77/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1107 - accuracy: 0.9800\n",
      "Epoch 00077: val_accuracy did not improve from 0.90476\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1049 - accuracy: 0.9881 - val_loss: 0.1690 - val_accuracy: 0.9048\n",
      "Epoch 78/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0748 - accuracy: 1.0000\n",
      "Epoch 00078: val_accuracy did not improve from 0.90476\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.1055 - accuracy: 0.9881 - val_loss: 0.1742 - val_accuracy: 0.9048\n",
      "Epoch 79/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0867 - accuracy: 1.0000\n",
      "Epoch 00079: val_accuracy did not improve from 0.90476\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1007 - accuracy: 0.9881 - val_loss: 0.1784 - val_accuracy: 0.9048\n",
      "Epoch 80/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1389 - accuracy: 0.9600\n",
      "Epoch 00080: val_accuracy did not improve from 0.90476\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.0979 - accuracy: 0.9762 - val_loss: 0.1749 - val_accuracy: 0.9048\n",
      "Epoch 81/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1054 - accuracy: 0.9600\n",
      "Epoch 00081: val_accuracy did not improve from 0.90476\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0973 - accuracy: 0.9762 - val_loss: 0.1717 - val_accuracy: 0.9048\n",
      "Epoch 82/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1064 - accuracy: 0.9800\n",
      "Epoch 00082: val_accuracy did not improve from 0.90476\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.0958 - accuracy: 0.9881 - val_loss: 0.1737 - val_accuracy: 0.9048\n",
      "Epoch 83/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1147 - accuracy: 0.9600\n",
      "Epoch 00083: val_accuracy did not improve from 0.90476\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0940 - accuracy: 0.9762 - val_loss: 0.1798 - val_accuracy: 0.9048\n",
      "Epoch 84/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1197 - accuracy: 0.9600\n",
      "Epoch 00084: val_accuracy did not improve from 0.90476\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0956 - accuracy: 0.9762 - val_loss: 0.1769 - val_accuracy: 0.9048\n",
      "Epoch 85/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0494 - accuracy: 1.0000\n",
      "Epoch 00085: val_accuracy did not improve from 0.90476\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.0965 - accuracy: 0.9762 - val_loss: 0.1785 - val_accuracy: 0.9048\n",
      "Epoch 86/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0534 - accuracy: 1.0000\n",
      "Epoch 00086: val_accuracy did not improve from 0.90476\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0910 - accuracy: 0.9762 - val_loss: 0.1632 - val_accuracy: 0.9048\n",
      "Epoch 87/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1144 - accuracy: 0.9800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00087: val_accuracy improved from 0.90476 to 0.95238, saving model to ./model\\iris-087-val0.9524.h5\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0912 - accuracy: 0.9881 - val_loss: 0.1554 - val_accuracy: 0.9524\n",
      "Epoch 88/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0858 - accuracy: 1.0000\n",
      "Epoch 00088: val_accuracy did not improve from 0.95238\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.0929 - accuracy: 0.9881 - val_loss: 0.1565 - val_accuracy: 0.9048\n",
      "Epoch 89/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0991 - accuracy: 0.9800\n",
      "Epoch 00089: val_accuracy did not improve from 0.95238\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0894 - accuracy: 0.9881 - val_loss: 0.1625 - val_accuracy: 0.9048\n",
      "Epoch 90/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0687 - accuracy: 1.0000\n",
      "Epoch 00090: val_accuracy did not improve from 0.95238\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0886 - accuracy: 0.9881 - val_loss: 0.1767 - val_accuracy: 0.9048\n",
      "Epoch 91/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0514 - accuracy: 0.9800\n",
      "Epoch 00091: val_accuracy did not improve from 0.95238\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0889 - accuracy: 0.9762 - val_loss: 0.1747 - val_accuracy: 0.9048\n",
      "Epoch 92/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0684 - accuracy: 0.9800\n",
      "Epoch 00092: val_accuracy did not improve from 0.95238\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0869 - accuracy: 0.9762 - val_loss: 0.1607 - val_accuracy: 0.9048\n",
      "Epoch 93/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0936 - accuracy: 0.9800\n",
      "Epoch 00093: val_accuracy did not improve from 0.95238\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.0925 - accuracy: 0.9881 - val_loss: 0.1524 - val_accuracy: 0.9048\n",
      "Epoch 94/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0916 - accuracy: 0.9800\n",
      "Epoch 00094: val_accuracy did not improve from 0.95238\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0853 - accuracy: 0.9881 - val_loss: 0.1583 - val_accuracy: 0.9048\n",
      "Epoch 95/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0574 - accuracy: 1.0000\n",
      "Epoch 00095: val_accuracy did not improve from 0.95238\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0864 - accuracy: 0.9881 - val_loss: 0.1709 - val_accuracy: 0.9048\n",
      "Epoch 96/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0605 - accuracy: 0.9800\n",
      "Epoch 00096: val_accuracy did not improve from 0.95238\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0848 - accuracy: 0.9762 - val_loss: 0.1653 - val_accuracy: 0.9048\n",
      "Epoch 97/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1003 - accuracy: 0.9800\n",
      "Epoch 00097: val_accuracy did not improve from 0.95238\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0834 - accuracy: 0.9762 - val_loss: 0.1550 - val_accuracy: 0.9048\n",
      "Epoch 98/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0678 - accuracy: 1.0000\n",
      "Epoch 00098: val_accuracy did not improve from 0.95238\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0820 - accuracy: 0.9881 - val_loss: 0.1506 - val_accuracy: 0.9048\n",
      "Epoch 99/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0509 - accuracy: 1.0000\n",
      "Epoch 00099: val_accuracy did not improve from 0.95238\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.0828 - accuracy: 0.9881 - val_loss: 0.1500 - val_accuracy: 0.9048\n",
      "Epoch 100/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1102 - accuracy: 0.9800\n",
      "Epoch 00100: val_accuracy did not improve from 0.95238\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0811 - accuracy: 0.9881 - val_loss: 0.1563 - val_accuracy: 0.9048\n",
      "Epoch 101/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0949 - accuracy: 0.9800\n",
      "Epoch 00101: val_accuracy did not improve from 0.95238\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0821 - accuracy: 0.9762 - val_loss: 0.1643 - val_accuracy: 0.9048\n",
      "Epoch 102/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0947 - accuracy: 0.9800\n",
      "Epoch 00102: val_accuracy did not improve from 0.95238\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.0810 - accuracy: 0.9762 - val_loss: 0.1573 - val_accuracy: 0.9048\n",
      "Epoch 103/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0993 - accuracy: 0.9600\n",
      "Epoch 00103: val_accuracy did not improve from 0.95238\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0811 - accuracy: 0.9762 - val_loss: 0.1506 - val_accuracy: 0.9048\n",
      "Epoch 104/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0908 - accuracy: 0.9800\n",
      "Epoch 00104: val_accuracy did not improve from 0.95238\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0788 - accuracy: 0.9881 - val_loss: 0.1513 - val_accuracy: 0.9048\n",
      "Epoch 105/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0666 - accuracy: 1.0000\n",
      "Epoch 00105: val_accuracy did not improve from 0.95238\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0792 - accuracy: 0.9881 - val_loss: 0.1544 - val_accuracy: 0.9048\n",
      "Epoch 106/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0905 - accuracy: 0.9800\n",
      "Epoch 00106: val_accuracy did not improve from 0.95238\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0782 - accuracy: 0.9881 - val_loss: 0.1514 - val_accuracy: 0.9048\n",
      "Epoch 107/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0649 - accuracy: 1.0000\n",
      "Epoch 00107: val_accuracy did not improve from 0.95238\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0777 - accuracy: 0.9881 - val_loss: 0.1516 - val_accuracy: 0.9048\n",
      "Epoch 108/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0952 - accuracy: 0.9800\n",
      "Epoch 00108: val_accuracy did not improve from 0.95238\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0771 - accuracy: 0.9881 - val_loss: 0.1496 - val_accuracy: 0.9048\n",
      "Epoch 109/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0513 - accuracy: 1.0000\n",
      "Epoch 00109: val_accuracy did not improve from 0.95238\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0768 - accuracy: 0.9881 - val_loss: 0.1469 - val_accuracy: 0.9048\n",
      "Epoch 110/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0620 - accuracy: 1.0000\n",
      "Epoch 00110: val_accuracy did not improve from 0.95238\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0777 - accuracy: 0.9881 - val_loss: 0.1486 - val_accuracy: 0.9048\n",
      "Epoch 111/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0863 - accuracy: 0.9800\n",
      "Epoch 00111: val_accuracy did not improve from 0.95238\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0786 - accuracy: 0.9881 - val_loss: 0.1454 - val_accuracy: 0.9048\n",
      "Epoch 112/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0950 - accuracy: 0.9800\n",
      "Epoch 00112: val_accuracy did not improve from 0.95238\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0751 - accuracy: 0.9881 - val_loss: 0.1512 - val_accuracy: 0.9048\n",
      "Epoch 113/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0522 - accuracy: 0.9800\n",
      "Epoch 00113: val_accuracy did not improve from 0.95238\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0758 - accuracy: 0.9762 - val_loss: 0.1567 - val_accuracy: 0.9048\n",
      "Epoch 114/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0521 - accuracy: 0.9800\n",
      "Epoch 00114: val_accuracy did not improve from 0.95238\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0756 - accuracy: 0.9762 - val_loss: 0.1509 - val_accuracy: 0.9048\n",
      "Epoch 115/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0526 - accuracy: 0.9800\n",
      "Epoch 00115: val_accuracy did not improve from 0.95238\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0741 - accuracy: 0.9762 - val_loss: 0.1439 - val_accuracy: 0.9048\n",
      "Epoch 116/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0398 - accuracy: 1.0000\n",
      "Epoch 00116: val_accuracy did not improve from 0.95238\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0740 - accuracy: 0.9881 - val_loss: 0.1412 - val_accuracy: 0.9524\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 117/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1005 - accuracy: 0.9800\n",
      "Epoch 00117: val_accuracy did not improve from 0.95238\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0747 - accuracy: 0.9881 - val_loss: 0.1400 - val_accuracy: 0.9524\n",
      "Epoch 118/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0706 - accuracy: 0.9800\n",
      "Epoch 00118: val_accuracy did not improve from 0.95238\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0749 - accuracy: 0.9881 - val_loss: 0.1418 - val_accuracy: 0.9524\n",
      "Epoch 119/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1001 - accuracy: 0.9800\n",
      "Epoch 00119: val_accuracy did not improve from 0.95238\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0722 - accuracy: 0.9881 - val_loss: 0.1515 - val_accuracy: 0.9048\n",
      "Epoch 120/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1063 - accuracy: 0.9600\n",
      "Epoch 00120: val_accuracy did not improve from 0.95238\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0738 - accuracy: 0.9762 - val_loss: 0.1612 - val_accuracy: 0.9048\n",
      "Epoch 121/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0920 - accuracy: 0.9800\n",
      "Epoch 00121: val_accuracy did not improve from 0.95238\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.0751 - accuracy: 0.9762 - val_loss: 0.1534 - val_accuracy: 0.9048\n",
      "Epoch 122/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0478 - accuracy: 0.9800\n",
      "Epoch 00122: val_accuracy did not improve from 0.95238\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0724 - accuracy: 0.9762 - val_loss: 0.1420 - val_accuracy: 0.9048\n",
      "Epoch 123/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0606 - accuracy: 1.0000\n",
      "Epoch 00123: val_accuracy did not improve from 0.95238\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0709 - accuracy: 0.9881 - val_loss: 0.1385 - val_accuracy: 0.9524\n",
      "Epoch 124/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1039 - accuracy: 0.9800\n",
      "Epoch 00124: val_accuracy did not improve from 0.95238\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.0743 - accuracy: 0.9881 - val_loss: 0.1387 - val_accuracy: 0.9048\n",
      "Epoch 125/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1050 - accuracy: 0.9800\n",
      "Epoch 00125: val_accuracy did not improve from 0.95238\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0739 - accuracy: 0.9881 - val_loss: 0.1400 - val_accuracy: 0.9524\n",
      "Epoch 126/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0401 - accuracy: 1.0000\n",
      "Epoch 00126: val_accuracy did not improve from 0.95238\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0706 - accuracy: 0.9881 - val_loss: 0.1507 - val_accuracy: 0.9048\n",
      "Epoch 127/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0302 - accuracy: 1.0000\n",
      "Epoch 00127: val_accuracy did not improve from 0.95238\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0741 - accuracy: 0.9762 - val_loss: 0.1560 - val_accuracy: 0.9048\n",
      "Epoch 128/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0961 - accuracy: 0.9600\n",
      "Epoch 00128: val_accuracy did not improve from 0.95238\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0751 - accuracy: 0.9762 - val_loss: 0.1423 - val_accuracy: 0.9048\n",
      "Epoch 129/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0799 - accuracy: 0.9800\n",
      "Epoch 00129: val_accuracy did not improve from 0.95238\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0698 - accuracy: 0.9881 - val_loss: 0.1394 - val_accuracy: 0.9524\n",
      "Epoch 130/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0904 - accuracy: 0.9800\n",
      "Epoch 00130: val_accuracy did not improve from 0.95238\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0702 - accuracy: 0.9881 - val_loss: 0.1387 - val_accuracy: 0.9524\n",
      "Epoch 131/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0374 - accuracy: 1.0000\n",
      "Epoch 00131: val_accuracy did not improve from 0.95238\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0694 - accuracy: 0.9881 - val_loss: 0.1413 - val_accuracy: 0.9048\n",
      "Epoch 132/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0418 - accuracy: 0.9800\n",
      "Epoch 00132: val_accuracy did not improve from 0.95238\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0691 - accuracy: 0.9762 - val_loss: 0.1433 - val_accuracy: 0.9048\n",
      "Epoch 133/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0529 - accuracy: 0.9800\n",
      "Epoch 00133: val_accuracy did not improve from 0.95238\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0697 - accuracy: 0.9762 - val_loss: 0.1422 - val_accuracy: 0.9048\n",
      "Epoch 134/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0882 - accuracy: 0.9600\n",
      "Epoch 00134: val_accuracy did not improve from 0.95238\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0709 - accuracy: 0.9762 - val_loss: 0.1377 - val_accuracy: 0.9524\n",
      "Epoch 135/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0779 - accuracy: 0.9800\n",
      "Epoch 00135: val_accuracy did not improve from 0.95238\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0684 - accuracy: 0.9881 - val_loss: 0.1384 - val_accuracy: 0.9524\n",
      "Epoch 136/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0470 - accuracy: 1.0000\n",
      "Epoch 00136: val_accuracy did not improve from 0.95238\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0691 - accuracy: 0.9881 - val_loss: 0.1414 - val_accuracy: 0.9048\n",
      "Epoch 137/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0465 - accuracy: 0.9800\n",
      "Epoch 00137: val_accuracy did not improve from 0.95238\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0681 - accuracy: 0.9762 - val_loss: 0.1401 - val_accuracy: 0.9048\n",
      "Epoch 138/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0396 - accuracy: 0.9800\n",
      "Epoch 00138: val_accuracy did not improve from 0.95238\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0676 - accuracy: 0.9762 - val_loss: 0.1374 - val_accuracy: 0.9524\n",
      "Epoch 139/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0727 - accuracy: 0.9800\n",
      "Epoch 00139: val_accuracy did not improve from 0.95238\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0680 - accuracy: 0.9881 - val_loss: 0.1368 - val_accuracy: 0.9524\n",
      "Epoch 140/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0727 - accuracy: 0.9800\n",
      "Epoch 00140: val_accuracy did not improve from 0.95238\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0672 - accuracy: 0.9881 - val_loss: 0.1386 - val_accuracy: 0.9524\n",
      "Epoch 141/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0709 - accuracy: 0.9800\n",
      "Epoch 00141: val_accuracy did not improve from 0.95238\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0667 - accuracy: 0.9762 - val_loss: 0.1409 - val_accuracy: 0.9048\n",
      "Epoch 142/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0389 - accuracy: 1.0000\n",
      "Epoch 00142: val_accuracy did not improve from 0.95238\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0708 - accuracy: 0.9762 - val_loss: 0.1433 - val_accuracy: 0.9048\n",
      "Epoch 143/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0896 - accuracy: 0.9600\n",
      "Epoch 00143: val_accuracy did not improve from 0.95238\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0689 - accuracy: 0.9762 - val_loss: 0.1363 - val_accuracy: 0.9524\n",
      "Epoch 144/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0435 - accuracy: 1.0000\n",
      "Epoch 00144: val_accuracy did not improve from 0.95238\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0697 - accuracy: 0.9881 - val_loss: 0.1363 - val_accuracy: 0.9524\n",
      "Epoch 145/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0476 - accuracy: 1.0000\n",
      "Epoch 00145: val_accuracy did not improve from 0.95238\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0661 - accuracy: 0.9881 - val_loss: 0.1359 - val_accuracy: 0.9524\n",
      "Epoch 146/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0356 - accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00146: val_accuracy did not improve from 0.95238\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0676 - accuracy: 0.9881 - val_loss: 0.1358 - val_accuracy: 0.9524\n",
      "Epoch 147/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0732 - accuracy: 0.9800\n",
      "Epoch 00147: val_accuracy did not improve from 0.95238\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0660 - accuracy: 0.9881 - val_loss: 0.1379 - val_accuracy: 0.9048\n",
      "Epoch 148/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1006 - accuracy: 0.9800\n",
      "Epoch 00148: val_accuracy did not improve from 0.95238\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0654 - accuracy: 0.9881 - val_loss: 0.1462 - val_accuracy: 0.9048\n",
      "Epoch 149/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0863 - accuracy: 0.9800\n",
      "Epoch 00149: val_accuracy did not improve from 0.95238\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0676 - accuracy: 0.9762 - val_loss: 0.1472 - val_accuracy: 0.9048\n",
      "Epoch 150/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0768 - accuracy: 0.9800\n",
      "Epoch 00150: val_accuracy did not improve from 0.95238\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0665 - accuracy: 0.9762 - val_loss: 0.1384 - val_accuracy: 0.9048\n",
      "Epoch 151/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0328 - accuracy: 1.0000\n",
      "Epoch 00151: val_accuracy did not improve from 0.95238\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0649 - accuracy: 0.9881 - val_loss: 0.1356 - val_accuracy: 0.9524\n",
      "Epoch 152/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0419 - accuracy: 1.0000\n",
      "Epoch 00152: val_accuracy did not improve from 0.95238\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0651 - accuracy: 0.9881 - val_loss: 0.1376 - val_accuracy: 0.9524\n",
      "Epoch 153/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0678 - accuracy: 0.9800\n",
      "Epoch 00153: val_accuracy did not improve from 0.95238\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0687 - accuracy: 0.9881 - val_loss: 0.1363 - val_accuracy: 0.9048\n",
      "Epoch 154/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0441 - accuracy: 1.0000\n",
      "Epoch 00154: val_accuracy did not improve from 0.95238\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0666 - accuracy: 0.9881 - val_loss: 0.1399 - val_accuracy: 0.9048\n",
      "Epoch 155/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0821 - accuracy: 0.9600\n",
      "Epoch 00155: val_accuracy did not improve from 0.95238\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0651 - accuracy: 0.9762 - val_loss: 0.1444 - val_accuracy: 0.9048\n",
      "Epoch 156/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0413 - accuracy: 0.9800\n",
      "Epoch 00156: val_accuracy did not improve from 0.95238\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0666 - accuracy: 0.9762 - val_loss: 0.1502 - val_accuracy: 0.9048\n",
      "Epoch 157/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0810 - accuracy: 0.9800\n",
      "Epoch 00157: val_accuracy did not improve from 0.95238\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0668 - accuracy: 0.9762 - val_loss: 0.1405 - val_accuracy: 0.9048\n",
      "Epoch 158/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0682 - accuracy: 0.9800\n",
      "Epoch 00158: val_accuracy did not improve from 0.95238\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0674 - accuracy: 0.9881 - val_loss: 0.1360 - val_accuracy: 0.9048\n",
      "Epoch 159/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0774 - accuracy: 0.9800\n",
      "Epoch 00159: val_accuracy did not improve from 0.95238\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0662 - accuracy: 0.9881 - val_loss: 0.1360 - val_accuracy: 0.9048\n",
      "Epoch 160/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0401 - accuracy: 1.0000\n",
      "Epoch 00160: val_accuracy did not improve from 0.95238\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.0683 - accuracy: 0.9762 - val_loss: 0.1386 - val_accuracy: 0.9048\n",
      "Epoch 161/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0393 - accuracy: 0.9800\n",
      "Epoch 00161: val_accuracy did not improve from 0.95238\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0640 - accuracy: 0.9762 - val_loss: 0.1385 - val_accuracy: 0.9048\n",
      "Epoch 162/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0789 - accuracy: 0.9800\n",
      "Epoch 00162: val_accuracy did not improve from 0.95238\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0629 - accuracy: 0.9881 - val_loss: 0.1359 - val_accuracy: 0.9524\n",
      "Epoch 163/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1006 - accuracy: 0.9800\n",
      "Epoch 00163: val_accuracy did not improve from 0.95238\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0628 - accuracy: 0.9881 - val_loss: 0.1361 - val_accuracy: 0.9048\n",
      "Epoch 164/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0674 - accuracy: 0.9800\n",
      "Epoch 00164: val_accuracy did not improve from 0.95238\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0638 - accuracy: 0.9881 - val_loss: 0.1360 - val_accuracy: 0.9048\n",
      "Epoch 165/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0661 - accuracy: 0.9800\n",
      "Epoch 00165: val_accuracy did not improve from 0.95238\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0625 - accuracy: 0.9881 - val_loss: 0.1367 - val_accuracy: 0.9524\n",
      "Epoch 166/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0394 - accuracy: 1.0000\n",
      "Epoch 00166: val_accuracy did not improve from 0.95238\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0633 - accuracy: 0.9881 - val_loss: 0.1419 - val_accuracy: 0.9048\n",
      "Epoch 167/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0398 - accuracy: 0.9800\n",
      "Epoch 00167: val_accuracy did not improve from 0.95238\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0634 - accuracy: 0.9762 - val_loss: 0.1398 - val_accuracy: 0.9048\n",
      "Epoch 168/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0720 - accuracy: 0.9800\n",
      "Epoch 00168: val_accuracy did not improve from 0.95238\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0627 - accuracy: 0.9881 - val_loss: 0.1360 - val_accuracy: 0.9524\n",
      "Epoch 169/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0833 - accuracy: 0.9800\n",
      "Epoch 00169: val_accuracy did not improve from 0.95238\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0639 - accuracy: 0.9881 - val_loss: 0.1365 - val_accuracy: 0.9048\n",
      "Epoch 170/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0808 - accuracy: 0.9800\n",
      "Epoch 00170: val_accuracy did not improve from 0.95238\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0621 - accuracy: 0.9881 - val_loss: 0.1362 - val_accuracy: 0.9524\n",
      "Epoch 171/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0878 - accuracy: 0.9800\n",
      "Epoch 00171: val_accuracy did not improve from 0.95238\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0614 - accuracy: 0.9881 - val_loss: 0.1372 - val_accuracy: 0.9524\n",
      "Epoch 172/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0931 - accuracy: 0.9600\n",
      "Epoch 00172: val_accuracy did not improve from 0.95238\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0613 - accuracy: 0.9762 - val_loss: 0.1394 - val_accuracy: 0.9048\n",
      "Epoch 173/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0340 - accuracy: 0.9800\n",
      "Epoch 00173: val_accuracy did not improve from 0.95238\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0625 - accuracy: 0.9762 - val_loss: 0.1393 - val_accuracy: 0.9048\n",
      "Epoch 174/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0933 - accuracy: 0.9600\n",
      "Epoch 00174: val_accuracy did not improve from 0.95238\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0623 - accuracy: 0.9762 - val_loss: 0.1361 - val_accuracy: 0.9524\n",
      "Epoch 175/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0803 - accuracy: 0.9800\n",
      "Epoch 00175: val_accuracy did not improve from 0.95238\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0636 - accuracy: 0.9881 - val_loss: 0.1369 - val_accuracy: 0.9048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 176/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0892 - accuracy: 0.9800\n",
      "Epoch 00176: val_accuracy did not improve from 0.95238\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0608 - accuracy: 0.9881 - val_loss: 0.1366 - val_accuracy: 0.9524\n",
      "Epoch 177/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0846 - accuracy: 0.9800\n",
      "Epoch 00177: val_accuracy did not improve from 0.95238\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0603 - accuracy: 0.9881 - val_loss: 0.1388 - val_accuracy: 0.9048\n",
      "Epoch 178/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0332 - accuracy: 0.9800\n",
      "Epoch 00178: val_accuracy did not improve from 0.95238\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0618 - accuracy: 0.9762 - val_loss: 0.1407 - val_accuracy: 0.9048\n",
      "Epoch 179/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0203 - accuracy: 1.0000\n",
      "Epoch 00179: val_accuracy did not improve from 0.95238\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0623 - accuracy: 0.9762 - val_loss: 0.1372 - val_accuracy: 0.9524\n",
      "Epoch 180/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0755 - accuracy: 0.9800\n",
      "Epoch 00180: val_accuracy did not improve from 0.95238\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0599 - accuracy: 0.9881 - val_loss: 0.1389 - val_accuracy: 0.9524\n",
      "Epoch 181/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0788 - accuracy: 0.9800\n",
      "Epoch 00181: val_accuracy did not improve from 0.95238\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0616 - accuracy: 0.9881 - val_loss: 0.1418 - val_accuracy: 0.9048\n",
      "Epoch 182/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0325 - accuracy: 1.0000\n",
      "Epoch 00182: val_accuracy did not improve from 0.95238\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0621 - accuracy: 0.9881 - val_loss: 0.1372 - val_accuracy: 0.9048\n",
      "Epoch 183/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0785 - accuracy: 0.9800\n",
      "Epoch 00183: val_accuracy did not improve from 0.95238\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0592 - accuracy: 0.9881 - val_loss: 0.1378 - val_accuracy: 0.9048\n",
      "Epoch 184/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0740 - accuracy: 0.9800\n",
      "Epoch 00184: val_accuracy did not improve from 0.95238\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0615 - accuracy: 0.9762 - val_loss: 0.1443 - val_accuracy: 0.9048\n",
      "Epoch 185/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0894 - accuracy: 0.9600\n",
      "Epoch 00185: val_accuracy did not improve from 0.95238\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0636 - accuracy: 0.9762 - val_loss: 0.1390 - val_accuracy: 0.9048\n",
      "Epoch 186/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0354 - accuracy: 0.9800\n",
      "Epoch 00186: val_accuracy did not improve from 0.95238\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0603 - accuracy: 0.9762 - val_loss: 0.1369 - val_accuracy: 0.9524\n",
      "Epoch 187/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0358 - accuracy: 1.0000\n",
      "Epoch 00187: val_accuracy did not improve from 0.95238\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0590 - accuracy: 0.9881 - val_loss: 0.1385 - val_accuracy: 0.9048\n",
      "Epoch 188/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0327 - accuracy: 1.0000\n",
      "Epoch 00188: val_accuracy did not improve from 0.95238\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0597 - accuracy: 0.9881 - val_loss: 0.1404 - val_accuracy: 0.9524\n",
      "Epoch 189/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0828 - accuracy: 0.9800\n",
      "Epoch 00189: val_accuracy did not improve from 0.95238\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0604 - accuracy: 0.9881 - val_loss: 0.1385 - val_accuracy: 0.9048\n",
      "Epoch 190/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0835 - accuracy: 0.9800\n",
      "Epoch 00190: val_accuracy did not improve from 0.95238\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0584 - accuracy: 0.9881 - val_loss: 0.1375 - val_accuracy: 0.9524\n",
      "Epoch 191/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0778 - accuracy: 0.9800\n",
      "Epoch 00191: val_accuracy did not improve from 0.95238\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0601 - accuracy: 0.9762 - val_loss: 0.1424 - val_accuracy: 0.9048\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#ModelCheckpoint; 모델 저장하는 함수\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import os\n",
    "model_save_folder = './model/'\n",
    "if not os.path.exists(model_save_folder):\n",
    "    os.mkdir(model_save_folder) #절대 경로 필요\n",
    "file = model_save_folder + 'iris-{epoch:03d}-val{val_accuracy:.4f}.h5'\n",
    "early_stopping = EarlyStopping(patience=40)\n",
    "checkpoint = ModelCheckpoint(filepath=file, monitor='val_accuracy',\n",
    "                            verbose=1, save_best_only=True)\n",
    "\n",
    "#2. 모델 구성\n",
    "model = Sequential()\n",
    "model.add(Dense(60, input_dim=4, activation='relu'))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(30, activation='relu'))\n",
    "model.add(Dense(3, activation='softmax')) #다중분류이므로 softmax \n",
    "\n",
    "#3. 학습 과정\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "#4. 학습\n",
    "fit_hist = model.fit(train_X, train_Y, batch_size=50, epochs=300,\n",
    "                    validation_split=0.2,\n",
    "                    callbacks=[early_stopping, checkpoint])\n",
    "\n",
    "#5. 모델 학습과정 표시\n",
    "fig, loss_ax = plt.subplots()\n",
    "loss_ax.plot(fit_hist.history['loss'], 'y', label='train loss')\n",
    "loss_ax.plot(fit_hist.history['val_loss'], 'g', label='val loss')\n",
    "loss_ax.set_xlabel('epochs')\n",
    "loss_ax.set_ylabel('loss')\n",
    "\n",
    "acc_ax = loss_ax.twinx() #x축을 공유하는 acc_ax\n",
    "acc_ax.plot(fit_hist.history['accuracy'], 'b', label='train accuracy')\n",
    "acc_ax.plot(fit_hist.history['val_accuracy'], 'r', label='val accuracy')\n",
    "acc_ax.set_ylabel('accuracy')\n",
    "\n",
    "loss_ax.legend(loc='upper left')\n",
    "acc_ax.legend(loc='lower left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0425 - accuracy: 1.0000\n",
      "loss: 0.04251512512564659\n",
      "accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "#6. 모델 평가\n",
    "score = model.evaluate(test_X, test_Y, batch_size=10)\n",
    "print('loss:', score[0])\n",
    "print('accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1], dtype=int64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(np.array([[6.0,3.4,4.5,1.6]])).argmax(axis=1) #versicolor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#7. 예측\n",
    "real = np.argmax(test_Y, axis=1) #실제 값\n",
    "pred = model.predict(test_X).argmax(axis=1) #예측 값\n",
    "np.all(pred==real) #100% 동일하면 True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>real</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pred</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "real   0   1   2\n",
       "pred            \n",
       "0     14   0   0\n",
       "1      0  18   0\n",
       "2      0   0  13"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ct_result = pd.crosstab(pred, real)\n",
    "ct_result.index.name='pred'\n",
    "ct_result.columns.name='real'\n",
    "ct_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
