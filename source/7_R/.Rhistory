#stringsAsFactors=F; 문자를 chr 타입으로 하기 위함(factor 타입X)
head(df_word, 10)
str(df_word, 10)
str(df_word)
library(dplyr)
df_word <-rename(df_word, word=Var1, freq=Freq)
str(df_word)
head(df_word)
nrow(df_word)
df_word %>%
filter(nchar(word)>=2)
df_word <-filter(df_word, nchar(word)>=2)
head(df_word)
#자주 사용되는 단어 빈도표 top20 만들기
top20 <-df_word[order(-df_word$freq), ][1:20]
#자주 사용되는 단어 빈도표 top20 만들기
top20 <-df_word[order(-df_word$freq), ][1:20,]
top20
top20 <-df_word %>%
arrange(desc(freq)) %>%
head(20)
top20
#자주 사용되는 단어 top20 그래프 그리기
library(ggplot2)
ggplot(data=top20, aes(x=freq, y=reorder(word, freq))) +
geom_col()
ggplot(data=top20, aes(x=freq, y=reorder(word, freq))) +
geom_col() +
geom_text(aes(label=freq), hjust=1.2, col="white") +
labs(title="자주 사용되는 단어 top20",
x="출현 빈도", y="출현 단어")
ggplot(data=top20, aes(x=freq, y=reorder(word, freq))) +
geom_col() +
geom_text(aes(label=freq), hjust=1.2, col="white") +
labs(title="자주 사용되는 단어 top20",
subtitle="(힙합가사)"
x="출현 빈도", y="출현 단어")
ggplot(data=top20, aes(x=freq, y=reorder(word, freq))) +
geom_col() +
geom_text(aes(label=freq), hjust=1.2, col="white") +
labs(title="자주 사용되는 단어 top20",
subtitle="(힙합가사)",
x="출현 빈도", y="출현 단어")
ggplot(data=top20, aes(x=freq, y=reorder(word, freq))) +
geom_col() +
geom_text(aes(label=freq), hjust=1.2, col="white") +
labs(title="힙합 가사에 자주 사용되는 단어 top20",
x="출현 빈도", y="출현 단어")
install.packages("wordcloud")
library(wordcloud)
round(runif(6, min=1, max=45))
display.brewer.all()
?wordcloud
pal <-brewer.pal(8, "Dark2")
set.seed(1234)
wordcloud(word=df_word$word, #출력될 단어
freq=df_word$freq, #단어 빈도
min.freq=2,        #최소 단어 빈도
max.words=200,     #표현 단어 수
random.order = F,  #고빈도 단어 중앙 배치
rot.per=.1,        #회전 단어 비율
scale=c(2,0.3),    #단어 크기 범위
colors=pal)        #색깔 목록 지정
wordcloud(words=df_word$word,
freq=df_word$freq,
min.freq = 2,
max.words=200,
random.order = F,
ort.per=.1,
scale=c(2,0.3),
colors=rainbow(16)) #pal대신 rainbow, topo.colors 등 사용 가능
wordcloud(words=df_word$word,
freq=df_word$freq,
min.freq = 2,
max.words=200,
random.order = F,
ort.per=.1,
scale=c(2,0.3),
colors=pal) #pal대신 rainbow, topo.colors 등 사용 가능
# 2. 국정원 트윗 데이터 텍스트 마이닝
twitter <-read.csv("inData/twitter.csv",
header=T,
stringsAsFactors = F,
fileEncoding = 'UTF-8')
head(twitter)
View(twitter)
nouns <-extractNoun(twitter$tw)
head(nouns)
class(nouns)
wordcount <-table(unlist(nouns))
class(wordcount)
df_word <-as.data.frame(wordcount, stringsAsFactors = FALSE)
df_word <-rename(df_word, word=Var1, freq=Freq)
head(df_word)
wordcount <-table(unlist(nouns))
class(wordcount)
df_word <-as.data.frame(wordcount, stringsAsFactors = FALSE)
head(df_word)
nouns <- extractNoun(twitter$tw)
head(nouns)
class(nouns)
wordcount <- table(unlist(nouns))
class(wordcount)
df_word <-as.data.frame(wordcount, stringsAsFactors=FALSE)
df_word <-rename(df_word, word=Var1, freq=Freq)
str(df_word)
head(df_word)
# 2. 국정원 트윗 데이터 텍스트 마이닝
twitter <-read.csv("inData/twitter.csv",
header=T,
stringsAsFactors = F,
fileEncoding = 'UTF-8')
head(twitter)
nouns <- extractNoun(twitter$tw)
head(nouns)
class(nouns)
wordcount <- table(unlist(nouns))
class(wordcount)
df_word <-as.data.frame(wordcount, stringsAsFactors=FALSE)
df_word <-rename(df_word, word=Var1, freq=Freq)
head(nouns)
df_word <-as.data.frame(wordcount, stringsAsFactors=FALSE)
head(df_word)
# 2. 국정원 트윗 데이터 텍스트 마이닝
twitter <-read.csv("inData/twitter.csv",
header=T,
stringsAsFactors = F,
fileEncoding = 'UTF-8')
head(twitter)
View(twitter)
nouns <- extractNoun(twitter$tw)
head(nouns)
class(nouns)
wordcount <- table(unlist(nouns))
nouns
nouns <- extractNoun(twitter$tw)
nouns <- extractNoun(twitter$tw)
# 2. 국정원 트윗 데이터 텍스트 마이닝
twitter <-read.csv("inData/twitter.csv",
header=T,
stringsAsFactors = F,
fileEncoding = 'UTF-8')
head(twitter)
View(twitter)
nouns <- extractNoun(twitter$tw)
# 2. 국정원 트윗 데이터 텍스트 마이닝
twitter <-read.csv("inData/twitter.csv",
header=T,
stringsAsFactors = F,
fileEncoding = 'UTF-8')
head(twitter)
class(twitter)
twitter <-rename(twitter,
no=번호,
id=계정이름,
date=작성일,
tw=내용)
edit(twitter)
#필요 없는 문자, 단어 삭제하기
twitter$tw <-str_replace_all(twitter$tw, '\\W', ' ')
twitter$tw <-str_replace_all(twitter$tw, '[ㄱ-ㅎ]', ' ')
twitter$tw <-str_replace_all(twitter$tw, '  ', ' ')
twitter$tw <-str_replace_all(twitter$tw, '  ', ' ') #스페이스 줄이기기
head(twitter)
nouns <-extractNoun(twitter$tw)
head(nouns)
class(nouns)
wordcount <-table(unlist(nouns))
class(wordcount)
df_word <-as.data.frame(wordcount, stringsAsFactors = F)
df_word <-rename(df_word, word=Var1, freq=Freq)
str(df_word)
head(df_word)
#출현단어 중 2글자 이상만 분석
df_word <-filter(df_word, nchar(word)>1)
#최빈 단어 top20 출력, 그래프
top 20 <-df_word[order(df_word$freq, decreasing = T), ][1:20, ]
top20
#최빈 단어 top20 출력, 그래프
top20 <-df_word[order(df_word$freq, decreasing = T), ][1:20, ]
top20
#최빈 단어 top20 출력, 그래프
top20 <-df_word[order(df_word$freq, decreasing = T), ][1:20, ]
top20
df_word %>%
arrange(desc(freq)) %>%
head(20) %>%
ggplot(aes(x=freq, y=reorder(word, freq))) +
geom_bar(stat="identity")
df_word %>%
arrange(desc(freq)) %>%
head(20) %>%
ggplot(aes(x=freq, y=reorder(word, freq))) +
geom_bar(stat="identity") +
labs(x="출현 빈도", y="출현 단어")
df_word %>%
arrange(desc(freq)) %>%
head(20) %>%
ggplot(aes(x=freq, y=reorder(word, freq))) +
geom_bar(stat="identity") +
labs(x="빈도", y="출현 단어")
ggplot(top20, aes(x=freq, y=reorder(word, freq))) +
geom_col() +
labs(x="출현 빈도", y="출현 단어") +
geom_text(aes(label=freq), hjust=1.2, col="white")
#워드클라우드 그리기
set.seed(1234)
pal <-brewer.pal(9, "Blues")[5:9]
wordcloud(word=df_word$word,
freq=df_word$freq,
min.freq = 5,
max.words = 300,
random.order = F,
rot.per=0.1,
scale=c(3, 0.3),
colors=pal)
sort(table(twitter$id))
more150Id <-twitter %>%
group_by(id) %>%
summarise(n=n()) %>%
filter(n>150)
more150 <- twitter %>%
filter(id %in% more150Id$id) %>%
select(id, tw)
table(more150$id)
more150$tw <-str_replace_all(more150$tw, '\\W', ' ')
more150$tw <-str_replace_all(more150$tw, '[ㄱ-ㅎ]', ' ')
more150$tw <-str_replace_all(more150$tw, '  ', ' ')
nouns <-extractNoun(more150$tw)
head(nouns)
class(nouns)
wordcount <-table(unlist(nouns))
class(wordcount)
df_word <-as.data.frame(wordcount, stringsAsFactors = F)
head(df_word)
df_word <-rename(df_word, word=Var1, freq=Freq)
str(df_word)
df_word <-filter(df_word, nchar(word)>1)
wordcloud(word=df_word$word,
freq=df_word$freq,
min.freq = 5,
max.words = 200,
random.order = F,
rot.per=0.1,
sclae=c(3, 0.3),
color=pal)
wordcloud(word=df_word$word,
freq=df_word$freq,
min.freq = 5,
max.words = 300,
random.order = F,
rot.per=0.1,
sclae=c(3, 0.3),
color=pal)
df_word %>%
arrange(-freq) %>%
head(20)
df_word %>%
arrange(-freq) %>%
head(20) %>%
ggplot(aes(x=freq, y=reorder(word, freq))) +
geom_col() +
geom_text(aes(label=freq), col="yellow", hjust=1.1) +
labs(y="단어", x="출현 빈도 수",
title="150회 이상 트윗한 아이디 게시물의 최빈 단어")
set.seed(1234)
wordcloud(word=df_word$word,
freq=df_word$freq,
min.freq = 5,
max.words = 300,
random.order = F,
rot.per=0.1,
sclae=c(3, 0.3),
color=pal)
library(rvest)
# 2021-02-19 R_웹 데이터 수집      ⓒcherryuki(ji) #
# # # 13장. 웹 데이터 수집(웹크롤링) # # #
# 1. 정적 웹크롤링: 단일 페이지 크롤링 (rvest 패키지 사용)
install.packages("rvest")
library(rvest)
url <-'https://movie.naver.com/movie/point/af/list.nhn'
text <-read_html(url, encoding='CP949')
text
#영화 제목; .movie
nodes <-html_nodes(text, '.movie')
nodes
title <-html_text(nodes)
title
#해당 영화 페이지
movieInfo <-html_attr(nodes, 'href')
movieInfo <-paste0(url, movieInfo)
movieInfo
#평점; .list_netizen_score em
nodes <-html_nodes(text, ".list_netizen_score em")
nodes
point <-html_text(nodes)
point
#영화 리뷰; .title
nodes <-html_nodes(text, '.title')
nodes
review <-html_text(nodes, trim=TRUE)
review <-gsub('\t', '', review)
review <-gsub('\n', '', review)
review
page <-cbind(title, movieInfo)
page <-cbind(page, point)
strsplit(review, '중[0-9]{1,2}')
review <-unlist(strsplit(review, '중[0-9]{1,2}'))[seq(2,20,2)]
review <-gsub('신고', '', review)
review
page <-cbind(page, review)
View(page)
#csv파일로 out
write.csv(page, "outData/movie_review.csv")
write.csv(page, "outData/movie_review.csv", row.names=F)
#여러 페이지 정적 웹크롤링
home <- 'https://movie.naver.com/movie/point/af/list.nhn'
site <- 'https://movie.naver.com/movie/point/af/list.nhn?&page='
movie.review <-NULL
for(i in 1:100) {
url <-paste0(site, i);
text <-read_html(url, encoding='CP949')
#영화제목; .movie
nodes <-html_nodes(text, '.movie')
title <-html_text(nodes)
#해당 영화 페이지
movieInfo <-html_attr(nodes, 'href')
movieInfo <-paste0(home, movieInfo)
#평점; .list_netizen_score em
nodes <-html_nodes(text, ".list_netizen_score em")
point <-html_text(nodes)
#영화 리뷰; .title
nodes <-html_nodes(text, '.title')
review <-html_text(nodes, trim=TRUE)
review <-gsub('\t', '', review)
review <-gsub('\n', '', review)
review <- unlist(strsplit(review, '중[0-9]{1,2}'))[seq(2,20,2)]
review <- gsub('신고', '', review)
page <-cbind(title, movieInfo)
page <-cbind(page, point)
page <-cbind(page, review)
movie.review <-rbind(movie.review, page)
}
View(movie.review)
write.csv(movie.review, 'outData/movie_review100page.csv', row.names=F)
#1~100페이지까지 크롤링한 영화들 중 평점이 높은 10개를 시각화
movie <-as.data.frame(movie.review, stringsAsFactors = F)
str(movie)
movie$point <-as.numeric(movie$point)
library(dplyr)
library(ggplot2)
movie %>%
group_by(title) %>%
summarise(avg.point=mean(point),
sum.point=sum(point)) %>%
arrange(desc(avg.point, sum.point)) %>%
head(10)
more5 <-movie %>%
group_by(title) %>%
summarise(n=n()) %>%
filter(n>=5)
more5[order(-more5$n),]
movie %>%
filter(title %in% more5$title) %>%
group_by(title) %>%
summarise(avg.point=mean(point)) %>%
arrange(desc(avg.point)) %>%
head(10)
movie %>%
filter(title %in% more5$title) %>%
group_by(title) %>%
summarise(avg.point=mean(point)) %>%
arrange(desc(avg.point)) %>%
head(10) %>%
ggplot(aes(x=avg.point, y=reorder(title, avg.point))) +
geom_col()
movie %>%
filter(title %in% more5$title) %>%
group_by(title) %>%
summarise(avg.point=mean(point)) %>%
arrange(desc(avg.point)) %>%
head(10) %>%
ggplot(aes(x=avg.point, y=reorder(title, avg.point))) +
geom_col() +
labs(title="평점이 높은 top10 영화",
subtitle="(평점 최소 5개 이상인 영화 대상)",
x="평점", y="영화 제목") +
geom_text(aes(label=round(avg.point,1)), col="white", hjust=1.2)
library(stringr)
#영화 리뷰 내용만 뽑아 최빈단어 10개, 워드 칼라우드 시각화
useNIADic()
#영화 리뷰 내용만 뽑아 최빈단어 10개, 워드 칼라우드 시각화
useNIADic()
#영화 리뷰 내용만 뽑아 최빈단어 10개, 워드 칼라우드 시각화
library(KoNLP)
useNIADic()
movie$review
review <-movie$review
review <-str_replace_all(review, '\\W', ' ')
review <-str_replace_all(review, '[ㄱ-ㅎ]', ' ')
review <-str_replace_all(review, '  ', ' ')
review <-str_replace_all(review, '  ', ' ')
review <-str_replace_all(review, '  ', ' ')
nouns <-extractNoun(review)
head(nouns)
wordcount <-table(unlist(nouns))
df_word <-as.data.frame(wordcount, stringsAsFactors = F)
head(df_word)
df_word <-rename(df_word, word_Var1, freq=Freq)
df_word <-rename(df_word, word_Var1, freq=Freq)
df_word <-rename(df_word, word=Var1, freq=Freq)
df_word <-df_word %>% filter(nchar(word)>1)
top10 <-df_word %>%
arrange(-freq) %>%
head(10)
ggplot(top10, aes(x=freq, y=reorder(word, freq))) +
geom_bar(stat="identity")
library(wordcloud)
display.brewer.all()
set.seed(1234)
pal <-brewer.pal(8, 'Set2')
wordcloud(words=df_word$word,
freq=df_word$freq,
min.freq=5,
max.words=200,
random.order = F,
rot.per=0.1,
scale=c(3, 0.3),
color=pal)
library(httr)
library(rvest)
library(RSelenium)
# 필요한 패키지 다운로드와 로드
install.packages("RSelenium")
library(RSelenium)
###셀레니움 동적 웹 크롤링 준비 완료
#예제1. 특정 부분에 text를 입력한 후 엔터한 결과를 크롤링(검색창)
remDr <-remoteDriver(port=4445L, #포트번호
browserName='chrome') #사용할 브라우저
remDr$open()
remDr$navigate('https://www.youtube.com')
webElem <-remDr$findElement(using='css', '#search')
webElem$sendKeysToElement(list('과학 다큐 비욘드', key='enter'))
#현재 페이지의 html 소스 가져오기
html <-remDr$getPageSource()[[1]]
html <-read_html(html)
#'#video-title' css 안의 text 가져오기
youtube_title <-html %>%  html_nodes('#video-title') %>% html_text()
youtube_title[1:5]
head(youtube_title)
youtube_url <- html %>%  html_nodes('#video-title') %>% html_attr('href')
head(youtube_url)
youtube_url <- ifelse(is.na(youtube_url), '',
paste0('https://www.youtube.com'), youtube_url)
#제목만 text파일로 out
write.table(youtube_title,
file='outData/과학다큐비욘드결과.txt',
sep='',
row.names=F,
quote=F)
result <-cbind(youtube_title, youtube_url)
write.csv(result, filte="outData/과학다큐비욘드결과.csv",
row.names=F)
#예제2. n번 마우스 스크롤 다운한 크롤링(댓글)
remD <-remoteDriver(port=4445L,
browserName='chorme')
remD$open()
#예제2. n번 마우스 스크롤 다운한 크롤링(댓글)
remD <-remoteDriver(port=4445L,
browserName='chorme')
remD$open()
#예제2. n번 마우스 스크롤 다운한 크롤링(댓글)
remD <-remoteDriver(port=4445L,
browserName='chrome')
remD$open()
remD$navigate('https://youtu.be/tZooW6PritE')
btn <-remD$findElement(using='css selector',
value='.html15-main-video')
btn <- remD$findElement(using='css selector',
value='.html5-main-video')
btn$clickElement()
#마우스 스크롤 다운
remD$executeScript("window.scrollTo(0,500)")
remD$executeScript("window.scrollTo(0,1000")
remD$executeScript("window.scrollTo(0,1000)")
remD$executeScript("window.scrollTo(1000,1500)")
remD$executeScript("window.scrollTo(1000,2000)")
remD$executeScript("window.scrollTo(0,3000)")
remD$executeScript("window.scrollTo(0,5000)")
#현재 페이지의 html 소스 가져오기
html <-remD$getPageSource()[1]
#현재 페이지의 html 소스 가져오기
html <-remD$getPageSource()[[1]]
html <-read_html(html)
comments <-html %>% html_nodes('#content-text') %>%  html_text()
comments[1:10]
#마우스 스크롤 다운
remD$executeScript("window.scrollTo(0,500)")
remD$executeScript("window.scrollTo(0,1000)")
remD$executeScript("window.scrollTo(1000,1500)")
remD$executeScript("window.scrollTo(1000,2000)")
remD$executeScript("window.scrollTo(0,3000)")
remD$executeScript("window.scrollTo(0,5000)")
#현재 페이지의 html 소스 가져오기
html <-remD$getPageSource()[[1]]
html <-read_html(html)
comments <-html %>% html_nodes('#content-text') %>%  html_text()
comments[1:10]
write.table(comments, file="outData/댓글.txt",
sep=',',
#row.names=F,
quote=F)
