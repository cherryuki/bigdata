{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01. 자료수집\n",
    "### 1) 뉴스 기사 웹크롤링\n",
    "### 2) 식품 리포트 위주 크롤링 \n",
    "- pdf파일은 어려움(txt파일 가능)\n",
    "\n",
    "### 3) 통계청, data 위주 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#식음료신문: http://www.thinkfood.co.kr/news/articleList.html?sc_section_code=S1N1&view_type=sm\n",
    "#div.list-titles > a 기사링크\n",
    "#기사제목: div.list-titles > a > strong \n",
    "#기사 페이지 기사 제목: div.article-head-title\n",
    "#기사 내용: article.article-veiw-body\n",
    "\n",
    "#농수축산신문(식품): http://www.aflnews.co.kr/news/articleList.html?sc_section_code=S1N5&view_type=sm\n",
    "#기사제목: h4.titles > a 텍스트\n",
    "#기사 링크: h4.titles > a href\n",
    "#기사 내용: #article-view-content-div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#식음료신문: http://www.thinkfood.co.kr/news/articleList.html?sc_section_code=S1N1&view_type=sm\n",
    "#div.list-titles > a 기사링크\n",
    "#기사제목: div.list-titles > a > strong \n",
    "#기사 페이지 기사 제목: div.article-head-title\n",
    "#기사 내용: article.article-veiw-body\n",
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "result_df = pd.DataFrame(None, columns = ['title', 'link', 'date', 'news'])\n",
    "num=1\n",
    "index=0\n",
    "for num in range(1, 150):\n",
    "    url = 'http://www.thinkfood.co.kr/news/articleList.html?page='+str(num)+'&total=25340&sc_section_code=S1N1&sc_sub_section_code=&sc_serial_code=&sc_area=&sc_level=&sc_article_type=&sc_view_level=&sc_sdate=&sc_edate=&sc_serial_number=&sc_word=&sc_word2=&sc_andor=&sc_order_by=E&view_type=sm'\n",
    "    html = requests.get(url)\n",
    "    soup = BeautifulSoup(html.content, 'html.parser')\n",
    "    link_list = soup.select('div.list-titles > a')\n",
    "    link_list = ['http://www.thinkfood.co.kr'+link.attrs['href'] for link in link_list]\n",
    "    date_list = soup.select('div.list-dated')\n",
    "    date_list = [date.text for date in date_list]\n",
    "    date_list = ' '.join(date_list)\n",
    "    date_list = re.findall(r'\\d{4}-\\d{2}-\\d{2}', date_list)\n",
    "    title_list = soup.select('div.list-titles > a > strong')\n",
    "    title_list = [title.text for title in title_list]\n",
    "    for i in range(len(date_list)):\n",
    "        html = requests.get(link_list[i])\n",
    "        news_soup = BeautifulSoup(html.content, 'html.parser')\n",
    "        news_list = news_soup.select('article.article-veiw-body')\n",
    "        news_list = [news.text for news in news_list]\n",
    "        result_df.loc[index] = [title_list[i], link_list[i], date_list[i], news_list]\n",
    "        index+=1\n",
    "    num+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.to_csv('01_식음료신문.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#농수축산신문(식품): http://www.aflnews.co.kr/news/articleList.html?page=1&total=5118&box_idxno=&sc_section_code=S1N5&view_type=sm\n",
    "#기사제목: h4.titles > a 텍스트\n",
    "#기사 링크: h4.titles > a href\n",
    "#기사 내용: #article-view-content-div\n",
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "result_df2 = pd.DataFrame(None, columns = ['title', 'link', 'date', 'news'])\n",
    "num=1\n",
    "index=0\n",
    "for num in range(1,50):\n",
    "    url = 'http://www.aflnews.co.kr/news/articleList.html?page='+str(num)+'&total=5118&box_idxno=&sc_section_code=S1N5&view_type=sm'\n",
    "    html = requests.get(url)\n",
    "    soup = BeautifulSoup(html.content, 'html.parser')\n",
    "    link_list = soup.select('h4.titles > a')\n",
    "    link_list = ['http://www.aflnews.co.kr'+link.attrs['href'] for link in link_list]\n",
    "    title_list = soup.select('h4.titles > a')\n",
    "    title_list = [title.text for title in title_list]\n",
    "    date_list = soup.select('span.byline')\n",
    "    date_list = [date.text for date in date_list]\n",
    "    date_list = ' '.join(date_list)\n",
    "    date_list = re.findall(r'\\d{4}.\\d{2}.\\d{2}', date_list)\n",
    "    for i in range(len(date_list)):\n",
    "        html = requests.get(link_list[i])\n",
    "        news_soup = BeautifulSoup(html.content, 'html.parser')\n",
    "        news_date = news_soup.select_one('i.icon-clock-o')\n",
    "        news_list = news_soup.select('article#article-view-content-div')\n",
    "        news_list = [news.text for news in news_list]\n",
    "        result_df2.loc[index] = [title_list[i], link_list[i], date_list[i], news_list]\n",
    "        index+=1\n",
    "    num+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df2.to_csv('02_농수축산신문.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#농수축산신문 이슈플러스\n",
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "result_df3 = pd.DataFrame(None, columns = ['title', 'link', 'date', 'news'])\n",
    "num=1\n",
    "index=0\n",
    "for num in range(1,26):\n",
    "    url = 'http://www.aflnews.co.kr/news/articleList.html?page='+str(num)+'&total=514&box_idxno=&sc_section_code=S1N11&view_type=sm'\n",
    "    html = requests.get(url)\n",
    "    soup = BeautifulSoup(html.content, 'html.parser')\n",
    "    link_list = soup.select('h4.titles > a')\n",
    "    link_list = ['http://www.aflnews.co.kr'+link.attrs['href'] for link in link_list]\n",
    "    title_list = soup.select('h4.titles > a')\n",
    "    title_list = [title.text for title in title_list]\n",
    "    date_list = soup.select('span.byline')\n",
    "    date_list = [date.text for date in date_list]\n",
    "    date_list = ' '.join(date_list)\n",
    "    date_list = re.findall(r'\\d{4}.\\d{2}.\\d{2}', date_list)\n",
    "    for i in range(len(date_list)):\n",
    "        html = requests.get(link_list[i])\n",
    "        news_soup = BeautifulSoup(html.content, 'html.parser')\n",
    "        news_date = news_soup.select_one('i.icon-clock-o')\n",
    "        news_list = news_soup.select('article#article-view-content-div')\n",
    "        news_list = [news.text for news in news_list]\n",
    "        result_df3.loc[index] = [title_list[i], link_list[i], date_list[i], news_list]\n",
    "        index+=1\n",
    "    num+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df3.to_csv('02_농수축산신문_축산.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#식품외식경제: http://www.aflnews.co.kr/news/articleList.html?page=1&total=5118&box_idxno=&sc_section_code=S1N5&view_type=sm\n",
    "#기사제목: div.list-titles > a 텍스트\n",
    "#기사 링크: div.list-titles > a href\n",
    "#날짜: div.list-dated \\d{4}-\\d{2}-\\d{2}\n",
    "#기사 내용: #article-view-content-div\n",
    "result_df4 = pd.DataFrame(None, columns = ['title', 'link', 'date', 'news'])\n",
    "num=1\n",
    "index=0\n",
    "for num in range(1, 150):\n",
    "    url = 'http://www.foodbank.co.kr/news/articleList.html?page='+str(num)+'&total=21032&sc_section_code=&sc_sub_section_code=S2N1&sc_serial_code=&sc_area=&sc_level=&sc_article_type=&sc_view_level=&sc_sdate=&sc_edate=&sc_serial_number=&sc_word=&sc_word2=&sc_andor=&sc_order_by=E&view_type=sm&sc_multi_code='\n",
    "    html = requests.get(url)\n",
    "    soup = BeautifulSoup(html.content, 'html.parser')\n",
    "    link_list = soup.select('div.list-titles > a')\n",
    "    link_list = ['http://www.foodbank.co.kr'+link.attrs['href'] for link in link_list]\n",
    "    date_list = soup.select('div.list-dated')\n",
    "    date_list = [date.text for date in date_list]\n",
    "    date_list = ' '.join(date_list)\n",
    "    date_list = re.findall(r'\\d{4}-\\d{2}-\\d{2}', date_list)\n",
    "    title_list = soup.select('div.list-titles > a > strong')\n",
    "    title_list = [title.text for title in title_list]\n",
    "    for i in range(len(date_list)):\n",
    "        html = requests.get(link_list[i])\n",
    "        news_soup = BeautifulSoup(html.content, 'html.parser')\n",
    "        news_list = news_soup.select('#article-view-content-div')\n",
    "        news_list = [news.text for news in news_list]\n",
    "        result_df4.loc[index] = [title_list[i], link_list[i], date_list[i], news_list]\n",
    "        index+=1\n",
    "    num+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df4.to_csv('03_식품외식경제.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#식품저널뉴스(전체): http://www.aflnews.co.kr/news/articleList.html?page=1&total=5118&box_idxno=&sc_section_code=S1N5&view_type=sm\n",
    "#기사제목: h4.titles > a 텍스트\n",
    "#기사 링크: h4.titles > a href\n",
    "#날짜 span.byline \\d{4}.\\d{2}.\\d{2}\n",
    "#기사 내용: #article-view-content-div\n",
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "result_df5 = pd.DataFrame(None, columns = ['title', 'link', 'date', 'news'])\n",
    "num=1\n",
    "index=0\n",
    "for num in range(1,300):\n",
    "    url = 'http://www.foodnews.co.kr/news/articleList.html?page='+str(num)+'&total=63398&box_idxno=&sc_section_code=S1N1&view_type=sm'\n",
    "    html = requests.get(url)\n",
    "    soup = BeautifulSoup(html.content, 'html.parser')\n",
    "    link_list = soup.select('h4.titles > a')\n",
    "    link_list = ['http://www.aflnews.co.kr'+link.attrs['href'] for link in link_list]\n",
    "    title_list = soup.select('h4.titles > a')\n",
    "    title_list = [title.text for title in title_list]\n",
    "    date_list = soup.select('span.byline')\n",
    "    date_list = [date.text for date in date_list]\n",
    "    date_list = ' '.join(date_list)\n",
    "    date_list = re.findall(r'\\d{4}.\\d{2}.\\d{2}', date_list)\n",
    "    for i in range(len(date_list)):\n",
    "        html = requests.get(link_list[i])\n",
    "        news_soup = BeautifulSoup(html.content, 'html.parser')\n",
    "        news_date = news_soup.select_one('i.icon-clock-o')\n",
    "        news_list = news_soup.select('article#article-view-content-div')\n",
    "        news_list = [news.text for news in news_list]\n",
    "        result_df5.loc[index] = [title_list[i], link_list[i], date_list[i], news_list]\n",
    "        index+=1\n",
    "    num+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df5.to_csv('04_식품저널뉴스.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
